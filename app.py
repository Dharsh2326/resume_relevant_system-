# Resume Relevance Check System
# Complete implementation with web interface

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import sqlite3
import json
import re
from typing import Dict, List, Tuple, Optional
import io
from pathlib import Path

# Document processing
try:
    import PyPDF2
    import docx
    from docx import Document
except ImportError:
    st.error("Missing required packages. Please install: pip install PyPDF2 python-docx")
    st.stop()

# NLP and similarity
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import nltk
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    from nltk.stem import PorterStemmer
except ImportError:
    st.error("Missing required packages. Please install: pip install scikit-learn nltk")
    st.stop()

# Download required NLTK data
@st.cache_resource
def download_nltk_data():
    try:
        nltk.download('punkt', quiet=True)
        nltk.download('stopwords', quiet=True)
        nltk.download('punkt_tab', quiet=True)
        return True
    except:
        return False

# Initialize database
def init_database():
    """Initialize SQLite database for storing results"""
    conn = sqlite3.connect('resume_evaluations.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS job_postings (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            description TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS evaluations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            job_id INTEGER,
            candidate_name TEXT,
            relevance_score REAL,
            verdict TEXT,
            missing_skills TEXT,
            feedback TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (job_id) REFERENCES job_postings (id)
        )
    ''')
    
    conn.commit()
    conn.close()

class DocumentProcessor:
    """Handles PDF and DOCX document processing"""
    
    @staticmethod
    def extract_text_from_pdf(pdf_file) -> str:
        """Extract text from PDF file"""
        try:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text.strip()
        except Exception as e:
            st.error(f"Error reading PDF: {str(e)}")
            return ""
    
    @staticmethod
    def extract_text_from_docx(docx_file) -> str:
        """Extract text from DOCX file"""
        try:
            doc = Document(docx_file)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text.strip()
        except Exception as e:
            st.error(f"Error reading DOCX: {str(e)}")
            return ""
    
    @staticmethod
    def extract_text(file) -> str:
        """Extract text from uploaded file"""
        if file.type == "application/pdf":
            return DocumentProcessor.extract_text_from_pdf(file)
        elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            return DocumentProcessor.extract_text_from_docx(file)
        else:
            st.error("Unsupported file format. Please upload PDF or DOCX files.")
            return ""

class TextProcessor:
    """Handles text preprocessing and analysis"""
    
    def __init__(self):
        self.stemmer = PorterStemmer()
        try:
            self.stop_words = set(stopwords.words('english'))
        except:
            self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])
    
    def preprocess_text(self, text: str) -> str:
        """Clean and preprocess text"""
        # Convert to lowercase
        text = text.lower()
        
        # Remove special characters and digits
        text = re.sub(r'[^a-zA-Z\s]', ' ', text)
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_skills(self, text: str) -> List[str]:
        """Extract skills from text using keyword matching"""
        # Common technical skills (expand this list as needed)
        skill_keywords = [
            'python', 'java', 'javascript', 'sql', 'html', 'css', 'react', 'angular',
            'node', 'mongodb', 'mysql', 'postgresql', 'aws', 'azure', 'docker',
            'kubernetes', 'git', 'machine learning', 'data science', 'pandas',
            'numpy', 'matplotlib', 'tensorflow', 'pytorch', 'excel', 'powerbi',
            'tableau', 'r', 'scala', 'spark', 'hadoop', 'linux', 'unix',
            'communication', 'leadership', 'problem solving', 'analytical',
            'teamwork', 'project management', 'agile', 'scrum'
        ]
        
        text_lower = text.lower()
        found_skills = []
        
        for skill in skill_keywords:
            if skill in text_lower:
                found_skills.append(skill)
        
        return list(set(found_skills))
    
    def extract_experience_years(self, text: str) -> Optional[int]:
        """Extract years of experience from text"""
        patterns = [
            r'(\d+)\s*(?:years?|yrs?)\s*(?:of\s*)?(?:experience|exp)',
            r'(\d+)\+\s*(?:years?|yrs?)',
            r'experience.*?(\d+)\s*(?:years?|yrs?)',
        ]
        
        text_lower = text.lower()
        for pattern in patterns:
            matches = re.findall(pattern, text_lower)
            if matches:
                return max([int(match) for match in matches])
        
        return None

class RelevanceAnalyzer:
    """Main class for analyzing resume relevance"""
    
    def __init__(self):
        self.text_processor = TextProcessor()
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
    
    def analyze_relevance(self, resume_text: str, jd_text: str) -> Dict:
        """Analyze relevance between resume and job description"""
        
        # Preprocess texts
        resume_clean = self.text_processor.preprocess_text(resume_text)
        jd_clean = self.text_processor.preprocess_text(jd_text)
        
        # Extract skills
        resume_skills = self.text_processor.extract_skills(resume_text)
        jd_skills = self.text_processor.extract_skills(jd_text)
        
        # Calculate skill match
        skill_match_score = self.calculate_skill_match(resume_skills, jd_skills)
        
        # Calculate semantic similarity
        semantic_score = self.calculate_semantic_similarity(resume_clean, jd_clean)
        
        # Calculate overall relevance score
        overall_score = (skill_match_score * 0.6 + semantic_score * 0.4) * 100
        
        # Determine verdict
        verdict = self.determine_verdict(overall_score)
        
        # Find missing skills
        missing_skills = list(set(jd_skills) - set(resume_skills))
        
        # Generate feedback
        feedback = self.generate_feedback(overall_score, missing_skills, resume_skills, jd_skills)
        
        return {
            'relevance_score': round(overall_score, 2),
            'skill_match_score': round(skill_match_score * 100, 2),
            'semantic_score': round(semantic_score * 100, 2),
            'verdict': verdict,
            'resume_skills': resume_skills,
            'jd_skills': jd_skills,
            'missing_skills': missing_skills,
            'feedback': feedback
        }
    
    def calculate_skill_match(self, resume_skills: List[str], jd_skills: List[str]) -> float:
        """Calculate skill match percentage"""
        if not jd_skills:
            return 1.0
        
        matched_skills = set(resume_skills) & set(jd_skills)
        return len(matched_skills) / len(jd_skills) if jd_skills else 0.0
    
    def calculate_semantic_similarity(self, resume_text: str, jd_text: str) -> float:
        """Calculate semantic similarity using TF-IDF and cosine similarity"""
        try:
            # Fit vectorizer on both texts
            corpus = [resume_text, jd_text]
            tfidf_matrix = self.vectorizer.fit_transform(corpus)
            
            # Calculate cosine similarity
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # Return similarity between resume and JD
            return similarity_matrix[0][1]
        except:
            return 0.0
    
    def determine_verdict(self, score: float) -> str:
        """Determine fit verdict based on score"""
        if score >= 75:
            return "High"
        elif score >= 50:
            return "Medium"
        else:
            return "Low"
    
    def generate_feedback(self, score: float, missing_skills: List[str], 
                         resume_skills: List[str], jd_skills: List[str]) -> str:
        """Generate personalized feedback for the candidate"""
        feedback = []
        
        if score >= 75:
            feedback.append("ğŸ‰ Excellent match! Your profile aligns well with the job requirements.")
        elif score >= 50:
            feedback.append("ğŸ‘ Good match! You have relevant skills but there's room for improvement.")
        else:
            feedback.append("âš ï¸ Your profile needs significant improvements to match this role.")
        
        if missing_skills:
            feedback.append(f"\nğŸ”§ Missing skills to focus on: {', '.join(missing_skills[:5])}")
        
        matched_skills = list(set(resume_skills) & set(jd_skills))
        if matched_skills:
            feedback.append(f"\nâœ… Your strong points: {', '.join(matched_skills[:5])}")
        
        # Additional suggestions based on score
        if score < 50:
            feedback.append("\nğŸ’¡ Suggestions:")
            feedback.append("- Consider taking courses in the missing skills")
            feedback.append("- Work on projects that demonstrate the required competencies")
            feedback.append("- Update your resume to better highlight relevant experience")
        
        return "\n".join(feedback)

def save_evaluation(job_id: int, candidate_name: str, result: Dict):
    """Save evaluation results to database"""
    conn = sqlite3.connect('resume_evaluations.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        INSERT INTO evaluations (job_id, candidate_name, relevance_score, verdict, missing_skills, feedback)
        VALUES (?, ?, ?, ?, ?, ?)
    ''', (
        job_id,
        candidate_name,
        result['relevance_score'],
        result['verdict'],
        json.dumps(result['missing_skills']),
        result['feedback']
    ))
    
    conn.commit()
    conn.close()

def save_job_posting(title: str, description: str) -> int:
    """Save job posting to database and return ID"""
    conn = sqlite3.connect('resume_evaluations.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        INSERT INTO job_postings (title, description)
        VALUES (?, ?)
    ''', (title, description))
    
    job_id = cursor.lastrowid
    conn.commit()
    conn.close()
    
    return job_id

def get_job_postings() -> List[Dict]:
    """Get all job postings from database"""
    conn = sqlite3.connect('resume_evaluations.db')
    cursor = conn.cursor()
    
    cursor.execute('SELECT id, title, description, created_at FROM job_postings ORDER BY created_at DESC')
    jobs = cursor.fetchall()
    conn.close()
    
    return [{'id': job[0], 'title': job[1], 'description': job[2], 'created_at': job[3]} for job in jobs]

def get_evaluations(job_id: Optional[int] = None) -> pd.DataFrame:
    """Get evaluations from database"""
    conn = sqlite3.connect('resume_evaluations.db')
    
    if job_id:
        query = '''
            SELECT e.*, j.title as job_title 
            FROM evaluations e 
            JOIN job_postings j ON e.job_id = j.id 
            WHERE e.job_id = ?
            ORDER BY e.relevance_score DESC
        '''
        df = pd.read_sql_query(query, conn, params=(job_id,))
    else:
        query = '''
            SELECT e.*, j.title as job_title 
            FROM evaluations e 
            JOIN job_postings j ON e.job_id = j.id 
            ORDER BY e.created_at DESC
        '''
        df = pd.read_sql_query(query, conn)
    
    conn.close()
    return df

# Streamlit Web Interface
def main():
    st.set_page_config(
        page_title="Resume Relevance Check System",
        page_icon="ğŸ“„",
        layout="wide"
    )
    
    # Download NLTK data
    download_nltk_data()
    
    # Initialize database
    init_database()
    
    st.title("ğŸ¯ Automated Resume Relevance Check System")
    st.markdown("*Developed for Innomatics Research Labs Placement Team*")
    
    # Sidebar navigation
    st.sidebar.title("Navigation")
    page = st.sidebar.selectbox(
        "Choose a page:",
        ["Upload & Analyze", "Job Postings", "Dashboard", "About"]
    )
    
    if page == "Upload & Analyze":
        upload_and_analyze_page()
    elif page == "Job Postings":
        job_postings_page()
    elif page == "Dashboard":
        dashboard_page()
    elif page == "About":
        about_page()

def upload_and_analyze_page():
    st.header("ğŸ“¤ Upload Resume & Analyze")
    
    # Get available job postings
    jobs = get_job_postings()
    
    if not jobs:
        st.warning("No job postings available. Please create a job posting first in the 'Job Postings' section.")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Select Job Position")
        job_options = {f"{job['title']} (ID: {job['id']})": job['id'] for job in jobs}
        selected_job = st.selectbox("Choose job position:", list(job_options.keys()))
        selected_job_id = job_options[selected_job]
        
        # Display job description
        selected_job_data = next(job for job in jobs if job['id'] == selected_job_id)
        st.text_area("Job Description:", selected_job_data['description'], height=200, disabled=True)
    
    with col2:
        st.subheader("Upload Resume")
        candidate_name = st.text_input("Candidate Name:", placeholder="Enter candidate's full name")
        uploaded_file = st.file_uploader(
            "Choose resume file:",
            type=['pdf', 'docx'],
            help="Upload PDF or DOCX format only"
        )
        
        if uploaded_file and candidate_name:
            if st.button("ğŸ” Analyze Resume", type="primary"):
                with st.spinner("Analyzing resume..."):
                    # Extract text from resume
                    resume_text = DocumentProcessor.extract_text(uploaded_file)
                    
                    if resume_text:
                        # Analyze relevance
                        analyzer = RelevanceAnalyzer()
                        result = analyzer.analyze_relevance(resume_text, selected_job_data['description'])
                        
                        # Save to database
                        save_evaluation(selected_job_id, candidate_name, result)
                        
                        # Display results
                        display_analysis_results(result, candidate_name)
                    else:
                        st.error("Failed to extract text from the uploaded file.")

def display_analysis_results(result: Dict, candidate_name: str):
    """Display analysis results"""
    st.success("âœ… Analysis Complete!")
    
    # Main metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Overall Score", f"{result['relevance_score']}/100")
    
    with col2:
        verdict_color = {"High": "ğŸŸ¢", "Medium": "ğŸŸ¡", "Low": "ğŸ”´"}
        st.metric("Verdict", f"{verdict_color.get(result['verdict'], '')} {result['verdict']}")
    
    with col3:
        st.metric("Skill Match", f"{result['skill_match_score']}/100")
    
    with col4:
        st.metric("Semantic Match", f"{result['semantic_score']}/100")
    
    # Detailed breakdown
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ¯ Skills Analysis")
        
        if result['resume_skills']:
            st.write("**Skills Found in Resume:**")
            for skill in result['resume_skills']:
                st.write(f"âœ… {skill}")
        else:
            st.write("No technical skills detected in resume.")
        
        if result['missing_skills']:
            st.write("**Missing Skills:**")
            for skill in result['missing_skills']:
                st.write(f"âŒ {skill}")
    
    with col2:
        st.subheader("ğŸ’¡ Personalized Feedback")
        st.write(result['feedback'])
    
    # Progress bars
    st.subheader("ğŸ“Š Score Breakdown")
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Skill Match Score**")
        st.progress(result['skill_match_score'] / 100)
    
    with col2:
        st.write("**Semantic Similarity Score**")
        st.progress(result['semantic_score'] / 100)

def job_postings_page():
    st.header("ğŸ“‹ Job Postings Management")
    
    tab1, tab2 = st.tabs(["Create New Job", "View Existing Jobs"])
    
    with tab1:
        st.subheader("â• Create New Job Posting")
        
        job_title = st.text_input("Job Title:", placeholder="e.g., Data Scientist, Software Engineer")
        job_description = st.text_area("Job Description:", height=300, 
                                     placeholder="Paste the complete job description here...")
        
        if st.button("ğŸ’¾ Save Job Posting", type="primary"):
            if job_title and job_description:
                job_id = save_job_posting(job_title, job_description)
                st.success(f"âœ… Job posting saved successfully! ID: {job_id}")
            else:
                st.error("Please fill in both job title and description.")
    
    with tab2:
        st.subheader("ğŸ“‘ Existing Job Postings")
        jobs = get_job_postings()
        
        if jobs:
            for job in jobs:
                with st.expander(f"{job['title']} - Created: {job['created_at'][:10]}"):
                    st.write(f"**Job ID:** {job['id']}")
                    st.write(f"**Title:** {job['title']}")
                    st.write(f"**Description:**")
                    st.write(job['description'])
        else:
            st.info("No job postings found. Create one using the 'Create New Job' tab.")

def dashboard_page():
    st.header("ğŸ“Š Dashboard & Analytics")
    
    # Get all evaluations
    evaluations_df = get_evaluations()
    
    if evaluations_df.empty:
        st.info("No evaluations found. Start by analyzing some resumes!")
        return
    
    # Summary statistics
    st.subheader("ğŸ“ˆ Summary Statistics")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Evaluations", len(evaluations_df))
    
    with col2:
        st.metric("Average Score", f"{evaluations_df['relevance_score'].mean():.1f}/100")
    
    with col3:
        high_verdicts = len(evaluations_df[evaluations_df['verdict'] == 'High'])
        st.metric("High Suitability", f"{high_verdicts} candidates")
    
    with col4:
        unique_jobs = evaluations_df['job_title'].nunique()
        st.metric("Active Jobs", unique_jobs)
    
    # Filters
    st.subheader("ğŸ” Filter Results")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        job_filter = st.selectbox("Filter by Job:", ['All'] + list(evaluations_df['job_title'].unique()))
    
    with col2:
        verdict_filter = st.selectbox("Filter by Verdict:", ['All', 'High', 'Medium', 'Low'])
    
    with col3:
        min_score = st.slider("Minimum Score:", 0, 100, 0)
    
    # Apply filters
    filtered_df = evaluations_df.copy()
    
    if job_filter != 'All':
        filtered_df = filtered_df[filtered_df['job_title'] == job_filter]
    
    if verdict_filter != 'All':
        filtered_df = filtered_df[filtered_df['verdict'] == verdict_filter]
    
    filtered_df = filtered_df[filtered_df['relevance_score'] >= min_score]
    
    # Display filtered results
    st.subheader("ğŸ“‹ Evaluation Results")
    if not filtered_df.empty:
        # Display table
        display_df = filtered_df[['candidate_name', 'job_title', 'relevance_score', 'verdict', 'created_at']].copy()
        display_df['created_at'] = pd.to_datetime(display_df['created_at']).dt.strftime('%Y-%m-%d %H:%M')
        st.dataframe(display_df, use_container_width=True)
        
        # Score distribution
        st.subheader("ğŸ“Š Score Distribution")
        score_bins = pd.cut(filtered_df['relevance_score'], bins=[0, 25, 50, 75, 100], 
                           labels=['0-25', '26-50', '51-75', '76-100'])
        score_dist = score_bins.value_counts()
        st.bar_chart(score_dist)
        
    else:
        st.info("No results match the selected filters.")

def about_page():
    st.header("â„¹ï¸ About the System")
    
    st.markdown("""
    ## ğŸ¯ Automated Resume Relevance Check System
    
    **Developed for Innomatics Research Labs Placement Team**
    
    ### ğŸš€ Features
    - **Automated Resume Analysis**: Upload resumes in PDF/DOCX format
    - **Intelligent Scoring**: Combines keyword matching with semantic analysis
    - **Skill Gap Analysis**: Identifies missing skills and provides feedback
    - **Verdict System**: High/Medium/Low suitability ratings
    - **Dashboard Analytics**: Track performance across multiple job postings
    - **Database Storage**: Persistent storage of all evaluations
    
    ### ğŸ› ï¸ Technical Implementation
    - **Document Processing**: PyPDF2 & python-docx for text extraction
    - **Text Analysis**: NLTK & scikit-learn for NLP processing
    - **Similarity Matching**: TF-IDF vectorization with cosine similarity
    - **Web Interface**: Streamlit for user-friendly interaction
    - **Database**: SQLite for data persistence
    
    ### ğŸ“Š Scoring Algorithm
    The relevance score is calculated using:
    - **60%** Skill Match Score (hard keyword matching)
    - **40%** Semantic Similarity Score (contextual understanding)
    
    ### ğŸ¯ Verdict Categories
    - **High (75-100)**: Excellent match, ready for interview
    - **Medium (50-74)**: Good match, minor skill gaps
    - **Low (0-49)**: Significant improvements needed
    
    ### ğŸ‘¥ Target Users
    - Placement Team Members
    - HR Recruiters
    - Hiring Managers
    - Students (for feedback)
    
    ### ğŸ“ˆ Business Impact
    - Reduces manual review time by 80%
    - Provides consistent evaluation criteria
    - Scales to handle thousands of applications
    - Offers actionable feedback to candidates
    """)

if __name__ == "__main__":
    main()